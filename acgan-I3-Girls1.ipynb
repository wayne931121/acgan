{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c57a917f-74e4-46a1-95f0-54e9a551b056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b6ebda2-dd82-4814-94a7-b679f6165a87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2dc5360a-2fde-4bd1-818b-7724765d7bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbfa16ed-93c1-456f-9b39-725279e3d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Opt:\n",
    "    def __init__(self):\n",
    "        self.n_epochs = 200\n",
    "        self.batch_size = 1\n",
    "        self.lr = 0.002\n",
    "        self.b1 = 0.5\n",
    "        self.b2 = 0.999\n",
    "        self.n_cpu = 8\n",
    "        self.latent_dim  = 100\n",
    "        self.n_classes = 10\n",
    "        self.img_size = 512\n",
    "        self.channels = 3\n",
    "        self.sample_interval = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "519edd39-d5e7-458e-980e-5c2cf62b0a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Opt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0498996f-3e56-4cbd-81d3-3f7db3617ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "#cuda = False\n",
    "\n",
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        self.label_emb = nn.Embedding(opt.n_classes, opt.latent_dim)\n",
    "\n",
    "        self.init_size = opt.img_size // 4  # Initial size before upsampling\n",
    "        self.l1 = nn.Sequential(nn.Linear(opt.latent_dim, 128 * self.init_size ** 2))\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(128, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(64, 0.8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, opt.channels, 3, stride=1, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        gen_input = torch.mul(self.label_emb(labels), noise)\n",
    "        out = self.l1(gen_input)\n",
    "        out = out.view(out.shape[0], 128, self.init_size, self.init_size)\n",
    "        img = self.conv_blocks(out)\n",
    "        return img\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, bn=True):\n",
    "            \"\"\"Returns layers of each discriminator block\"\"\"\n",
    "            block = [nn.Conv2d(in_filters, out_filters, 3, 2, 1), nn.LeakyReLU(0.2, inplace=True), nn.Dropout2d(0.25)]\n",
    "            if bn:\n",
    "                block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
    "            return block\n",
    "\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            *discriminator_block(opt.channels, 16, bn=False),\n",
    "            *discriminator_block(16, 32),\n",
    "            *discriminator_block(32, 64),\n",
    "            *discriminator_block(64, 128),\n",
    "        )\n",
    "\n",
    "        # The height and width of downsampled image\n",
    "        ds_size = opt.img_size // 2 ** 4\n",
    "\n",
    "        # Output layers\n",
    "        self.adv_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, 1), nn.Sigmoid())\n",
    "        self.aux_layer = nn.Sequential(nn.Linear(128 * ds_size ** 2, opt.n_classes), nn.Softmax())\n",
    "\n",
    "    def forward(self, img):\n",
    "        out = self.conv_blocks(img)\n",
    "        out = out.view(out.shape[0], -1)\n",
    "        validity = self.adv_layer(out)\n",
    "        label = self.aux_layer(out)\n",
    "\n",
    "        return validity, label\n",
    "\n",
    "\n",
    "# Loss functions\n",
    "adversarial_loss = torch.nn.BCELoss()\n",
    "auxiliary_loss = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c960737c-5437-4ad8-be0b-b15ba1d3c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator and discriminator\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "12ec0e2e-4ea8-4f08-bb70-87110990f3e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (conv_blocks): Sequential(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (2): Dropout2d(p=0.25, inplace=False)\n",
       "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (5): Dropout2d(p=0.25, inplace=False)\n",
       "    (6): BatchNorm2d(32, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (8): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (9): Dropout2d(p=0.25, inplace=False)\n",
       "    (10): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (11): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "    (12): LeakyReLU(negative_slope=0.2, inplace=True)\n",
       "    (13): Dropout2d(p=0.25, inplace=False)\n",
       "    (14): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (adv_layer): Sequential(\n",
       "    (0): Linear(in_features=131072, out_features=1, bias=True)\n",
       "    (1): Sigmoid()\n",
       "  )\n",
       "  (aux_layer): Sequential(\n",
       "    (0): Linear(in_features=131072, out_features=10, bias=True)\n",
       "    (1): Softmax(dim=None)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "    auxiliary_loss.cuda()\n",
    "\n",
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "040025ef-20ab-4554-abd6-90254f9e3686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.makedirs(\"images\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8c10610-37af-420d-b645-4154cf0d7447",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = r\"C:\\Users\\原神\\Downloads\\girls\"+\"\\\\\"\n",
    "name = \"girls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0409288b-d432-4420-9a99-92fa1a95522a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from PIL import Image\n",
    "\n",
    "size = (512, 512)\n",
    "\n",
    "for infile in os.listdir(p+\"val\\\\\"+name):\n",
    "        im = Image.open(p+\"val\\\\\"+name+\"\\\\\"+infile)\n",
    "        im = im.resize(size)\n",
    "        im = im.convert(\"RGB\")\n",
    "        os.remove(p+\"val\\\\\"+name+\"\\\\\"+infile)\n",
    "        im.save(p+\"val\\\\\"+name+\"\\\\\"+infile.replace(\"png\",\"jpg\"), \"JPEG\")\n",
    "\n",
    "for infile in os.listdir(p+\"train\\\\\"+name):\n",
    "        im = Image.open(p+\"train\\\\\"+name+\"\\\\\"+infile)\n",
    "        im = im.resize(size)\n",
    "        im = im.convert(\"RGB\")\n",
    "        os.remove(p+\"train\\\\\"+name+\"\\\\\"+infile)\n",
    "        im.save(p+\"train\\\\\"+name+\"\\\\\"+infile.replace(\"png\",\"jpg\"), \"JPEG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "46210b9d-2038-4268-b976-07aba98f8c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.ImageFolder(root=p,\n",
    "                               transform = transforms.Compose(\n",
    "                                   [\n",
    "                                       transforms.Resize(opt.img_size), \n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor(), \n",
    "                                       transforms.Normalize([0.5], [0.5])]),\n",
    "                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32afae7a-29de-434e-9a0b-ce06a3482505",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(\n",
    "    dataset,\n",
    "    batch_size=opt.batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed40b566-94a4-450e-85ad-7cd1d3aa885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.lr = 0.0002\n",
    "#opt.lr = 0.001\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2))\n",
    "\n",
    "FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "#scheduler2 = torch.optim.lr_scheduler.MultiStepLR(optimizer_D, milestones=[30,80], gamma=0.1)\n",
    "#scheduler1 = torch.optim.lr_scheduler.LinearLR(optimizer_G, start_factor=0.2, end_factor=0.02, total_iters=5000)\n",
    "#scheduler2 = torch.optim.lr_scheduler.LinearLR(optimizer_D, start_factor=0.2, end_factor=0.02, total_iters=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e7a88fb-679f-4561-b6b1-a4c7fc68284c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_image(n_row, batches_done):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
    "    # Sample noise\n",
    "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row ** 2, opt.latent_dim))))\n",
    "    # Get labels ranging from 0 to n_classes for n rows\n",
    "    labels = np.array([num for _ in range(n_row) for num in range(n_row)])\n",
    "    labels = Variable(LongTensor(labels))\n",
    "    gen_imgs = generator(z, labels)\n",
    "    save_image(gen_imgs.data, r\"C:\\Users\\原神\\Downloads\\result1\"+\"\\\\%d.png\" % batches_done, nrow=n_row, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03df9455-983c-49c8-96b7-78c4f7f34be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# opt.lr = 0.00002\n",
    "# #opt.lr = 0.1\n",
    "# #opt.lr = 0.01\n",
    "# # Optimizers\n",
    "# optimizer_G = torch.optim.SGD(generator.parameters(), lr=opt.lr, weight_decay=1e-5)\n",
    "# optimizer_D = torch.optim.SGD(discriminator.parameters(), lr=opt.lr, weight_decay=1e-5)\n",
    "\n",
    "# FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
    "# LongTensor = torch.cuda.LongTensor if cuda else torch.LongTensor\n",
    "\n",
    "# #scheduler1 = torch.optim.lr_scheduler.LinearLR(optimizer_G, start_factor=1e0, end_factor=1e-6, total_iters=128)\n",
    "# #scheduler2 = torch.optim.lr_scheduler.LinearLR(optimizer_D, start_factor=1e0, end_factor=1e-6, total_iters=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "923e48b4-9aa8-4a38-8c04-2dfbc56fc791",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a8ab6ad-43b0-4ccd-b5ca-b40c7521646e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batches_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3df6e9cc-f390-4843-9a58-e56a09e2622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.n_epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fed710e1-855e-4a69-9a2a-f1794c6d6a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer_G.param_groups[0]['lr'] = 1e-4\n",
    "# optimizer_D.param_groups[0]['lr'] = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ebf2c2cc-ca6e-448f-a42b-b8078e88ce90",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.sample_interval = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d3428dea-b820-4d93-9325-5831aeda0b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\原神\\AppData\\Local\\Temp\\ipykernel_12020\\1369393927.py:7: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\tensor\\python_tensor.cpp:80.)\n",
      "  valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
      "C:\\ai\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 25/5000] [Batch 14/17] [D loss: 1.237655, acc: 50%] [G loss: 1.621885] [D lr 2.00e-04][439]]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 67\u001b[0m\n\u001b[0;32m     64\u001b[0m     batches_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m batches_done \u001b[38;5;241m%\u001b[39m opt\u001b[38;5;241m.\u001b[39msample_interval \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     66\u001b[0m            \u001b[38;5;66;03m# sample_image(n_row=opt.n_classes, batches_done=batches_done)\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m            \u001b[43msample_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatches_done\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatches_done\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m#gc.collect()\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;66;03m#torch.cuda.empty_cache()\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m#scheduler1.step()\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m#scheduler2.step()\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 11\u001b[0m, in \u001b[0;36msample_image\u001b[1;34m(n_row, batches_done)\u001b[0m\n\u001b[0;32m      9\u001b[0m labels \u001b[38;5;241m=\u001b[39m Variable(LongTensor(labels))\n\u001b[0;32m     10\u001b[0m gen_imgs \u001b[38;5;241m=\u001b[39m generator(z, labels)\n\u001b[1;32m---> 11\u001b[0m \u001b[43msave_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_imgs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m原神\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mDownloads\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mresult1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;132;43;01m%d\u001b[39;49;00m\u001b[38;5;124;43m.png\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatches_done\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_row\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ai\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ai\\lib\\site-packages\\torchvision\\utils.py:147\u001b[0m, in \u001b[0;36msave_image\u001b[1;34m(tensor, fp, format, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_tracing():\n\u001b[0;32m    146\u001b[0m     _log_api_usage_once(save_image)\n\u001b[1;32m--> 147\u001b[0m grid \u001b[38;5;241m=\u001b[39m make_grid(tensor, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Add 0.5 after unnormalizing to [0, 255] to round to the nearest integer\u001b[39;00m\n\u001b[0;32m    149\u001b[0m ndarr \u001b[38;5;241m=\u001b[39m grid\u001b[38;5;241m.\u001b[39mmul(\u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39madd_(\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mclamp_(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m255\u001b[39m)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch\u001b[38;5;241m.\u001b[39muint8)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mC:\\ai\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mC:\\ai\\lib\\site-packages\\torchvision\\utils.py:98\u001b[0m, in \u001b[0;36mmake_grid\u001b[1;34m(tensor, nrow, padding, normalize, value_range, scale_each, pad_value)\u001b[0m\n\u001b[0;32m     96\u001b[0m             norm_range(t, value_range)\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 98\u001b[0m         \u001b[43mnorm_range\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_range\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensor should be of type torch.Tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\ai\\lib\\site-packages\\torchvision\\utils.py:92\u001b[0m, in \u001b[0;36mmake_grid.<locals>.norm_range\u001b[1;34m(t, value_range)\u001b[0m\n\u001b[0;32m     90\u001b[0m     norm_ip(t, value_range[\u001b[38;5;241m0\u001b[39m], value_range[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 92\u001b[0m     norm_ip(t, \u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mfloat\u001b[39m(t\u001b[38;5;241m.\u001b[39mmax()))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(opt.n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(dataloader):\n",
    "\n",
    "        batch_size = imgs.shape[0]\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = Variable(FloatTensor(batch_size, 1).fill_(1.0), requires_grad=False)\n",
    "        fake = Variable(FloatTensor(batch_size, 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "        # Configure input\n",
    "        real_imgs = Variable(imgs.type(FloatTensor))\n",
    "        labels = Variable(labels.type(LongTensor))\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise and labels as generator input\n",
    "        z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, opt.latent_dim))))\n",
    "        gen_labels = Variable(LongTensor(np.random.randint(0, opt.n_classes, batch_size)))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        validity, pred_label = discriminator(gen_imgs)\n",
    "        g_loss = 0.5 * (adversarial_loss(validity, valid) + auxiliary_loss(pred_label, gen_labels))\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Loss for real images\n",
    "        real_pred, real_aux = discriminator(real_imgs)\n",
    "        d_real_loss = (adversarial_loss(real_pred, valid) + auxiliary_loss(real_aux, labels)) / 2\n",
    "\n",
    "        # Loss for fake images\n",
    "        fake_pred, fake_aux = discriminator(gen_imgs.detach())\n",
    "        d_fake_loss = (adversarial_loss(fake_pred, fake) + auxiliary_loss(fake_aux, gen_labels)) / 2\n",
    "\n",
    "        # Total discriminator loss\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "\n",
    "        # Calculate discriminator accuracy\n",
    "        pred = np.concatenate([real_aux.data.cpu().numpy(), fake_aux.data.cpu().numpy()], axis=0)\n",
    "        gt = np.concatenate([labels.data.cpu().numpy(), gen_labels.data.cpu().numpy()], axis=0)\n",
    "        d_acc = np.mean(np.argmax(pred, axis=1) == gt)\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        print(\n",
    "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f, acc: %d%%] [G loss: %f] [D lr %.2e][%d]\"\n",
    "            % (epoch, opt.n_epochs, i, len(dataloader), d_loss.item(), 100 * d_acc, g_loss.item(),optimizer_D.param_groups[0]['lr'], batches_done)\n",
    "            ,end=\"\"\n",
    "        )\n",
    "        batches_done += 1\n",
    "        if batches_done % opt.sample_interval == 0:\n",
    "               # sample_image(n_row=opt.n_classes, batches_done=batches_done)\n",
    "               sample_image(n_row=3, batches_done=batches_done)\n",
    "        \n",
    "        #gc.collect()\n",
    "        #torch.cuda.empty_cache()\n",
    "    #scheduler1.step()\n",
    "    #scheduler2.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f87aacb-2bd4-44c7-b308-aa3391c3dbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler2.get_last_lr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de336ff2-0e94-4533-98fe-3725dedd4d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bcb4ba1-fb6f-44a8-8cb5-37ab80652dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image(n_row=3, batches_done=batches_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f49e05-ca55-4673-ad07-9ab2b507963e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# sample_image(n_row=opt.n_classes, batches_done=batches_done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff8ae25-0e17-4fa1-81d0-3c8598ca2af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = r\"C:\\Users\\TEST\\generator_girls512rgbclass10_1.pth\"\n",
    "dp = r\"C:\\Users\\TEST\\discriminator_girls512rgbclass10_1.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5255c631-0aaa-4ba2-8cd9-28e74adeb89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(generator.state_dict(), gp)\n",
    "torch.save(discriminator.state_dict(), dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f06c9ce-2588-4fba-bb6a-4d83305652bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a random image\n",
    "sample_image(n_row=1, batches_done=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804ab4e5-7fb1-450d-a4b8-c62a4b544937",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.load_state_dict(torch.load(dp, weights_only=True))\n",
    "discriminator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb3d1e2-4904-4fe5-af38-c654ab7196b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator.load_state_dict(torch.load(gp, weights_only=True))\n",
    "generator.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9b88aa-a7d0-439a-a849-2fc83a215891",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
